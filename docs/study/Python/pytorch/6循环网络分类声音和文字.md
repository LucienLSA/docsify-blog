

## 循环神经网络RNN分类声音

## 亦可定义gpu设备，在gpu上训练和测试。主要加在model和输入数据处

## 数据集为音频，分类结果为文件名的首位数字

### 加载声音数据集

raise RuntimeError(f"Couldn't find appropriate backend to handle uri {uri} and format {format}.")

1. 音频文件路径不正确
2. wav音频格式是不正确。
3. 缺少音频库，通过 pip install pysoundfile来安装音频库

```python
def load_data():
    import os
    import torchaudio

    xs = []
    ys = []

    #遍历文件夹下的所有文件
    for filename in os.listdir('data/superb'):

        #只要声音,过滤一些无关的文件
        if not filename.endswith('.wav'):
            continue

        #读取声音信息
        x = torchaudio.load('data/superb/%s' % filename)[0]
        x = x.reshape(-1, 1)

        #y来自文件名的第一个字符
        y = int(filename[0])

        xs.append(x)
        ys.append(y)

    return xs, ys


xs, ys = load_data()

len(xs), len(ys), xs[0].shape, ys[0]
```

### 定义数据集
```python
import torch

class Dataset(torch.utils.data.Dataset):

    def __len__(self):
        return len(xs)

    def __getitem__(self, i):
        return xs[i], ys[i]


dataset = Dataset()

x, y = dataset[0]

len(dataset), x.shape, y
```

### 定义数据集加载器
```python
loader = torch.utils.data.DataLoader(dataset=dataset,
                                     batch_size=8,
                                     shuffle=True,
                                     drop_last=True)

x, y = next(iter(loader))

len(loader), x.shape, y
```


### RNN 计算过程

![RNN](https://s2.loli.net/2024/05/14/eqmpt46YnkIrKJV.png)

计算过程是串行的，每一个时间点运算都依赖于上一个时间点。把每个时间点的记忆都保存下来，组成一个跟输入同样长度的序列

![模型计算](https://s2.loli.net/2024/05/14/JfSXILQeb6hzodP.png)

RNN将每一步记忆的维度进一步展开，最后一层将所有记忆展平，横着的向量则进行全连接输出，运算结果分类和回归。

### 定义神经网络模型的初始化部分
```python
class Model(torch.nn.Module):

    #模型初始化部分
    def __init__(self):
        super().__init__()

        #循环层
        self.rnn1 = torch.nn.RNN(input_size=1,
                                 hidden_size=16,
                                 batch_first=True)

        self.rnn2 = torch.nn.RNN(input_size=16,
                                 hidden_size=32,
                                 batch_first=True)

        self.rnn3 = torch.nn.RNN(input_size=32,
                                 hidden_size=64,
                                 batch_first=True)

        self.rnn4 = torch.nn.RNN(input_size=64,
                                 hidden_size=128,
                                 batch_first=True)

        #激活函数
        self.relu = torch.nn.ReLU()

        #池化层
        self.pool = torch.nn.AvgPool1d(kernel_size=7, stride=5)

        #线性输出
        self.fc = torch.nn.Linear(in_features=640, out_features=10)

    #定义神经网络计算过程
    def forward(self, x):

        #循环神经网络计算,抽取特征
        #[8, 4000, 1] -> [8, 4000, 16]
        x, _ = self.rnn1(x)
        x = self.relu(x)

        #池化,缩小数据规模,合并特征
        #[8, 4000, 16] -> [8, 799, 16]
        x = x.permute(0, 2, 1)
        x = self.pool(x)
        x = x.permute(0, 2, 1)

        #重复上面的计算
        #[8, 799, 16] -> [8, 159, 32]
        x, _ = self.rnn2(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1)
        x = self.pool(x)
        x = x.permute(0, 2, 1)

        #[8, 159, 32] -> [8, 31, 64]
        x, _ = self.rnn3(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1)
        x = self.pool(x)
        x = x.permute(0, 2, 1)

        #[8, 31, 64] -> [8, 5, 128]
        x, _ = self.rnn4(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1)
        x = self.pool(x)
        x = x.permute(0, 2, 1)

        #展平,准备线性计算
        #[8, 5, 128] -> [8, 640]
        x = x.flatten(start_dim=1)

        #线性计算输出
        #[8, 640] -> [8, 10]
        return self.fc(x)


model = Model()

model(torch.randn(8, 4000, 1)).shape
```

### 训练模型
```python
#训练
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fun = torch.nn.CrossEntropyLoss()
    model.train()

    for epoch in range(5):
        for i, (x, y) in enumerate(loader):

            out = model(x)
            loss = loss_fun(out, y)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if i % 1000 == 0:
                acc = (out.argmax(dim=1) == y).sum().item() / len(y)
                print(epoch, i, loss.item(), acc)

    torch.save(model, 'model/6.model')


train()
```

### 测试
```python
@torch.no_grad()
def test():
    model = torch.load('model/6.model')
    model.eval()

    correct = 0
    total = 0
    for i in range(100):
        x, y = next(iter(loader))

        out = model(x).argmax(dim=1)

        correct += (out == y).sum().item()
        total += len(y)

    print(correct / total)


test()
```


## 循环神经网络RNN分类文字sst

### 数据文件csv中，共有65000句话，每句话15个词，进行二分类任务

每一句由15个词，对应的词。即每一句话由15个数字组成。
字典文件为数字和词所对应表

### 加载数据文件
```python
import pandas as pd

#加载处理好的数据集,每句话是15个词,y是2分类,字典在data/sst2/vocab.txt
data = pd.read_csv('data/sst2/data.csv')
```

### 定义数据集进行封装
```python
import torch

class Dataset(torch.utils.data.Dataset):

    def __len__(self):
        return len(data)

    def __getitem__(self, i):
        #获取数据
        x, y = data.iloc[i]

        #以逗号分割x数据,转换为向量
        x = [int(i) for i in x.split(',')]
        x = torch.LongTensor(x)
        
        #y不需要太特别的处理
        y = int(y)

        return x, y


dataset = Dataset()

len(dataset), dataset[0]
```

### 数据集加载器
```python
loader = torch.utils.data.DataLoader(dataset=dataset,
                                     batch_size=8,
                                     shuffle=True,
                                     drop_last=True)

len(loader), next(iter(loader))
```

### 计算过程

![文本RNN计算过程](https://s2.loli.net/2024/05/14/Iawrd6Gi9oQyYeE.png)

RNN网络读一句话，一个词一个词读，读到最后一个词，把最后一个词的记忆作为一个特征向量，使用最后一个词的记忆计算一个输出的结果

### 定义RNN网络模型
```python
class Model(torch.nn.Module):

    #模型初始化部分
    def __init__(self):
        super().__init__()

        #词编码层,30522是词的数量,每个词会被编码为100维的向量
        self.embed = torch.nn.Embedding(num_embeddings=30522,
                                        embedding_dim=100)

        #RNN单元
        self.cell = torch.nn.GRUCell(input_size=100, hidden_size=512)

        #线性输出
        self.fc = torch.nn.Linear(in_features=512, out_features=2)

    #定义神经网络计算过程
    def forward(self, x):

        #每个词编码为100维的向量，每个词投影为一个向量
        #[8, 15] -> [8, 15, 100]
        x = self.embed(x)

        #初始记忆为空
        h = None

        #从前向后读句子中的每一个词
        for i in range(x.shape[1]):
            #[8, 100],[8, 512] -> [8, 512]
            # 每个词计算结果都是一个记忆
            h = self.cell(x[:, i], h)

        #根据最后一个词的记忆,分类整句话
        #[8, 512] -> [8, 2]
        return self.fc(h)

model = Model()

model(torch.ones(8, 15).long()).shape
```

### 训练
```python
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    loss_fun = torch.nn.CrossEntropyLoss()
    model.train()

    for epoch in range(2):
        for i, (x, y) in enumerate(loader):
            out = model(x)
            loss = loss_fun(out, y)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if i % 2000 == 0:
                acc = (out.argmax(dim=1) == y).sum().item() / len(y)
                print(epoch, i, loss.item(), acc)

    torch.save(model, 'model/7.model')

train()
```

### 测试
```python
@torch.no_grad()
def test():
    model = torch.load('model/7.model')
    model.eval()

    correct = 0
    total = 0
    for i in range(100):
        x, y = next(iter(loader))

        out = model(x).argmax(dim=1)

        correct += (out == y).sum().item()
        total += len(y)

    print(correct / total)


test()
```

